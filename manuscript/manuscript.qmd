---
title: |-
  Fact-Checking and Partisan Cheerleading: Dissemination Patterns of Political Fact-Checks on Twitter^[Author thanks to Pablo Barber√° for generously sharing data. Also, author thanks to Sakshi Bhalla, Mengqing (Maggie) Zhang, Nino Migineishvili, Jonghyun Lee, and Joyce Yanru Jiang for their help for the data collection. Additionally, author thanks to David Tewksbury, Tim Groeling, Briony Swire-Thompson, Eehyun Kim, Junmo Song, Jina Lee, and Soomin Hong for their helpful comments on the study.]
subtitle: |-
  {{wordcount}} words (excluding references)
filters:
  - wordcount
author:
  - name: "Je Hoon Chae^[Doctoral Student, Department of Communication, University of California, Los Angeles. email: \\href{mailto:chae@g.ucla.edu}{chae@g.ucla.edu}. webpage: \\href{https://jehoonchae.github.io}{https://jehoonchae.github.io}]"
  # - name: "Briony Swire-Thompson^[Assistant Professor, Department of Political Science, Northeastern University. email: \\href{mailto:b.swire-thompson@northeastern.edu}{b.swire-thompson@northeastern.edu}.]"
date: today
date-format: long
abstract: |
  \noindent In this study, we delve into the dynamics of political fact-checking news dissemination using an extensive dataset of PolitiFact articles and associated Twitter posts spanning from 2016 to 2021. We focus on how fact-checking content, whether aligned or conflicting with individual beliefs, penetrates across partisan divides. Our analysis reveals a strong tendency for fact-checking verdicts to favor Liberal/Democratic predispositions, a trend that amplifies during election cycles, while this trend doesn't directly imply a political bias in the fact-checking organization, since favorability distributions in published content cannot solely determine biases. Additionally, we find that Twitter users sharing fact-checking news predominantly align with liberal ideologies, regardless of the congruency of the fact-check result. The data shows an increase in the sharing of fact-checking content in alignment with users' political ideology during elections, with a significant spike among liberal users. Moreover, we unearth a substantial disparity in the sharing behavior among Twitter users, with a handful of heavy users dominating the dissemination landscape. Importantly, we uncover a clear connection between a user's political ideology and their tendency to selectively share content, a pattern especially pronounced among conservative users.
# papersize: a4
documentclass: article
fontsize: 12pt
geometry: 
  - left=1in 
  - right=1in
  - top=1in
  - bottom=1in
# format: docx
format:
  pdf:
    # toc: true
    number-sections: true
    colorlinks: true
    keep-tex: true
    pdf-engine: pdflatex
    # fig_caption: yes
    include-in-header: "../auxiliary/template.tex"
    # highlight-style: haddock
execute:
  warning: false
bibliography: "../auxiliary/references.bib"
# biblio-style: "../pnas.bst"
# csl: "../pnas.csl"
csl: "../auxiliary/apa.csl"
# csl: "../science-without-titles.csl"
link-citations: true
linkcolor: ucladarkblue
urlcolor: ucladarkblue
---

```{=tex}
\setlength{\parindent}{20pt}
\setlength{\parskip}{0pt}
```

\newpage

\doublespacing



<!-- The era of 'post-truth' politics has ushered in a surge of misinformation, necessitating a robust and vigilant response from a new battalion of fact-checkers [@lewandowsky2017beyond; @graves2016]. Organizations like *PolitiFact*, *FactCheck.org*, and the *Washington Post's Fact Checker* have become invaluable gatekeepers of truth, arduously sifting through statements, claims, and social media chatter to distinguish fact from fiction [@graves2016]. These efforts have been an increasingly important element in the broader media landscape, serving as a countermeasure to false information circulating through various media outlets (Nyhan and Reifler, 2015). -->

<!-- A rich body of empirical research in the social sciences highlights the efficacy of fact-checking as an instrument to correct political misperceptions [@lazer2018science; @graves2016]. Earlier, concerns about the so-called backfire effect were prevalent; the phenomenon where partisans strengthen their adherence to misinformation when faced with counter-attitudinal corrections [@nyhan2010corrections]. However, recent empirical evidence consistently demonstrates that fact-checking typically yields positive results [@swire2020they; @walter2020fact; @wood2019elusive; @swire2017processing]. In essence, it tends to diminish belief in misinformation while fostering trust in factual information. Moreover, these effects are not confined to instances where the fact-checking originates from in-group sources. Even when the source of the fact-check is an out-group organization, fact-checking continues to be effective [@chae2023perceiving]. Additionally, this effect proves resilient even when the fact-checking contradicts previously held biases [@coppock2023conceptual]. Hence, fact-checking stands as a potent force in correcting misperceptions and enhancing the factual accuracy of public discourse, irrespective of the source's political affiliation and the pre-existing biases of the recipients. -->

<!-- Contemporary political climate in the United States is characterized by an increasingly entrenched partisan divide, with each side often inhabiting a distinct and separate realm of facts. This particular trend has been noted by @achen2017democracy who have observed modern partisans entrenched within "their own facts". This political polarization extends beyond the realm of economic performance issues, including unemployment and inflation, permeating a broad range of political subjects [@jerit2012partisan, p.673]. Recent data supports this assertion, with polling consistently demonstrating a stark division in the perception of factual reality across partisan lines. For instance, a mere 21 percent of Republicans recognize President Joe Biden's electoral victory as legitimate, contrasted with over 90 percent of Democrats who affirm the validity of his win [@Cuthbert2022do]. This ideologically divided interpretation of reality contributes to a troubling cascade of downstream effects, such as heightened partisan animosity [@iyengar2012affect] and the tendency towards partisan-biased information processing [@taber2006motivated], ultimately undermining citizen competence [@kuklinski2001conceptual]. -->

<!-- Emerging research provides a beacon of hope amidst this ideologically fragmented landscape. Recent studies indicate that updating beliefs on pro-attitudinal political misinformation is possible, even among individuals who strongly identify with their partisan ideologies [@coppock2023conceptual; @wood2019elusive; @chae2023perceiving]. This is a promising breakthrough, suggesting that despite deep-rooted partisanship, people can revise their misconceptions when confronted with accurate information. This body of work raises the optimistic possibility of fostering dialogue and forming public opinion based on a similar and accurate factual foundation, regardless of ideological predispositions. Such a shared basis of factual understanding is a fundamental cornerstone for quality deliberation and is integral to the effective functioning of democratic processes [@fishkin2009people]. Consequently, these findings underscore the importance of strategies aimed at bridging the partisan fact-perception divide, as they have significant implications for the health and sustainability of American democratic processes. -->

<!-- However, the current state of affairs paints a different picture. This stark discrepancy prompts us to question: Despite the wealth of experimental evidence underscoring fact-checking's effectiveness in correcting partisan political misperceptions, why do partisan groups continue to inhabit vastly divergent factual realities, particularly concerning political (mis)information? A plausible explanation hinges on the concept of selective exposure [@sears1967selective], where partisans gravitate towards like-minded sources or content [@tyler2022partisan], thereby limiting their exposure to cross-cutting political information [@wojcieszak2009online]. Applied to the sphere of political fact-checking, this theory suggests that partisans are unlikely to engage with counter-attitudinal political fact-checking in the real world, regardless of its persuasive impact demonstrated in online survey experiments. Essentially, while fact-checkers and their organizations generate a trove of fact-checking articles to substantiate or debunk factual claims, this content might not successfully permeate the information bubbles of those who hold related political misperceptions.  -->

# Introduction

Fact-checking organizations serve a crucial role in our information landscape, tirelessly evaluating the truthfulness of statements made by public figures, celebrities, and relevant social media posts alike [@graves2016]. In the United States, a diverse range of organizations---from specialized fact-checking groups such as *PolitiFact*, *FactCheck.org*, and *Snopes*, to well-known news agencies like *CNN*, *The Washington Post*, *NPR*, and *The New York Times*---are committed to verifying the authenticity of these declarations. More specifically, organizations like *PolitiFact* devote considerable resources to evaluating the veracity of assertions within the political landscape. 

The process of claim adjudication in political sphere has undoubtedly ignited numerous debates, such as triggering partisan motivated reasoning or allegations of political bias contingent on the result of fact-checking. However, empirical evidence suggests that individuals reliably amend their perceptions in line with fact-checking outcomes [@walter2020fact; @porter2022political; @porter2022factual; @nyhan2020taking; @swire2020they; @swire2017processing]. This effect remains durable even when the information provided by the fact-check contradicts previously held beliefs [@coppock2023conceptual; @wood2019elusive] or the fact-checking source is considered to be an out-group [@chae2023perceiving]. Such findings offer a glimmer of optimism, suggesting that even staunch partisans may be capable of rational evaluation of disputed factual information. However, this raises the question: does exposure to cross-cutting fact-checking news genuinely occur in everyday life?

While a considerable body of academic research has assessed the effectiveness of fact-checking within controlled experimental settings, our grasp of fact-checking consumption within real-world contexts remains insufficient. One notable exception is the recent study by @guess2020exposure, which leveraged a mix of survey responses and web search tracking data. This investigation established that direct visits to fact-checking websites are extraordinarily infrequent, leaving the dynamics of cross-cutting exposure largely under-researched. As a consequence, individuals rarely access fact-checking news directly from specialized websites. This outcome implies that in real-world scenarios, social media arguably becomes the primary conduit for fact-checking news. This conclusion, based on empirical evidence from the @guess2020exposure study, underscores the crucial role of indirect or incidental exposure to fact-checking within social media platforms [@fletcher2018people]. However, preliminary research using social media data from platforms such as Twitter and Facebook suggests that these environments do not offer ideal conditions for cross-cutting information consumption [@bakshy2015exposure; @barbera2015tweeting; @conover2011political]. Rather, these platforms appear highly susceptible to partisan selective information sharing and exposure [@osmundsen2021partisan; @bowen2023learning], with fact-checking news not being immune to this pattern [@shin2017partisan]. If cross-cutting fact-checking exposure does not frequently occur in reality, then the meaningfulness of experimental findings demonstrating the robustness of correction effects [e.g., @wood2019elusive] against the widespread perspective of partisan motivated reasoning [@taber2006motivated] is called into question.

In this regards, our study aims to examine the degree to which political fact-checking news, either congruent (pro-attitudinal) or discordant (counter-attitudinal) with individuals' beliefs, permeates partisan boundaries. We aim to investigate whether this fact-checking content is shared within and across partisan divides, potentially contributing to a shared understanding of political reality. To achieve our research aims, we capitalized on a comprehensive dataset comprised of all fact-checking articles published by *PolitiFact* ($N = 9,523$)---a preeminent fact-checking organization---from January 1, 2016, to December 31, 2021. This corpus also encompassed the corresponding Twitter posts ($N = 35,965$) and affiliated retweet information for each article ($N_{\text{users}} = 153,797$; $N_{\text{retweet}} = 1,082,122$). Delving into this wealth of data, we distilled several key findings.

It was found that over 60\% of fact-checks deemed initial factual claims as false, a number of which were from sources associated with the Conservative/Republican Party. The study further discovered a greater volume of fact-checks congruent with Liberal/Democrats than with Conservative/Republicans throughout the period under investigation, with a notable increase in activity leading up to elections. When it came to dissemination, Twitter users sharing fact-checking news predominantly leaned towards liberal ideologies, a skew which remained even when conservative-leaning fact-checking content was included. Users sharing Liberal/Democrat congruent fact-checking echoed the overall distribution of fact-checking sharers, while conservative congruent fact-checking was relatively more shared among conservative-leaning users. The study also identified an increase in partisan-congruent sharing of fact-checking content during election periods, particularly among liberal users. The sharing of fact-checking posts was dominated by a small subset of heavy users, revealing a significant disparity in the sharing behavior among Twitter users. Lastly, the study highlighted selective sharing behavior, where a user's political ideology positively correlated with the extent of their selective sharing behavior, a trend more pronounced among conservative users.

<!-- Firstly, we identified a clear tendency for fact-checking verdicts to align with Liberal/Democratic predispositions, an inclination that is particularly pronounced during election cycles.^[Readers should recognize that the larger number of Liberal/Democrat-congruent fact-checking articles does not directly imply the political bias of the fact-checking organization. As @groeling2013media argues, media organization bias cannot be inferred only through the distribution of eventual favorableness in published content. In the context of fact-checking, the distribution of false claims made by each political side should be considered to identify organizational bias, which is beyond the scope of this study. This will be further discussed in the discussion section.] Secondly, our data showed that the majority of users disseminating these fact-checking posts identified as politically liberal. A third finding concerns conservative users, who, when interacting with fact-checking posts, predominantly selected those reinforcing their partisan biases‚Äîa reflection of strong partisan cheerleading in their sharing behaviors. Interestingly, this pattern of partisan cheerleading was also evident among liberal users, and it became especially apparent during election periods. Taken together, these findings illuminate the differences in factual reality perception across partisan lines and the implications for the dynamics governing the consumption and dissemination of political fact-checking information. These observations underscore the complexity of political fact-checking in a deeply polarized era, highlighting its potential impacts on public discourse and democratic deliberation. -->


<!-- Relatedly, using Twitter data from October 2012, @shin2017partisan demonstrated that partisans were far less likely to share fact-checking tweets that could presumably be interpreted as unfavorable to their in-group or favorable to their out-group. Conversely, fact-checking tweets that aligned with their political views were shared actively [@shin2017partisan]. That is, although the social engagement in Twitter related with political fact-checking is happening, the way it is occuring is not cross-cutting but closer to partisan cheerleading. -->

# Partisan Selective Sharing of Fact-Checking News on Social Media


We approach our understanding of how political fact-checking news is shared on social media through the lens of the two-step flow communication theory [@katz1955personal; @katz1957two]. We supplement this with insights from theories surrounding partisan-motivated information processing [@taber2006motivated; @stroud2010polarization; @kunda1990case]. The two-step flow communication theory posits that interpersonal relationships, rather than direct interaction with the original sources, usually convey media messages to the general public [@katz1957two]. In the realm of fact-checking news on social media, these intermediary "opinion leaders" [@lazarsfeld1944people] actively interact with fact-checking sites, digest fact-checked posts early, and circulate the examined information within their networks [@chadwick2011political; @kim2013stumbling]. Owing to their heightened political involvement, these individuals play a crucial role in spreading news on social media [@messing2014selective].

The emergence of social media platforms has magnified the importance of the two-step flow theory. Prior research has found that most social media users may not directly engage with fact-checking news, despite its extensive availability [@guess2020exposure]. Instead, opinion leaders or heavy social media users share this content within their networks, distributing the news [@bode2016political]. The interpretation and presentation by these opinion leaders significantly influence their network's acceptance or rejection of fact-checked information [@bakshy2015exposure]. Moreover, the two-step flow theory highlights the existence of echo chambers and information bubbles on social media. If opinion leaders mainly share fact-checking news that aligns with their ideological predispositions, their networks primarily get exposure to this selectively presented information. This dynamic fosters echo chambers and ideological polarization [@barbera2015social; @flaxman2016filter].

Twitter and Facebook, as social media platforms, have expanded the applicability of the two-step flow of communication theory. They serve as perfect environments for opinion leaders to convey information to a wide audience. For instance, a study on Twitter users during the 2016 U.S. election found that politically engaged individuals often spurred the spread of political information [@barbera2015tweeting]. Individuals less politically engaged then shared and consumed this distributed information. This pattern is a reflection of the two-step flow of communication theory, where opinion leaders get information from mass media and then broadcast it to their followers, shaping public opinion [@barbera2015tweeting]. Additionally, it was revealed that Facebook users with strong partisan beliefs were more likely to share news articles that bolstered their perspectives [@wells2016coproduction]. These partisan users disseminate news within their social networks, influencing their less politically active friends' and followers' views. This action represents the first step in the two-step flow. The explicit display of the two-step flow of communication on both Twitter and Facebook reaffirms the theory's relevance in today's digital media landscape.

So, what are the psychological factors that drive individuals to selectively share news on social media? Essentially, users' news sharing behavior on social media is a self-presentation exercise [@kraft2020social; @walther1996computer; @gantz1979people]. Users often share news while imagining an audience they intend to influence with their shared content [@litt2016imagined; @litt2012knock]. The criteria for determining the perceived worthiness of news content to share can vary across individuals and contexts [@karnowski2021worth; @kumpel2015news]. However, within our study's scope---sharing behavior related to political fact-checking---we posit that partisanship is the primary driver. Viewing this behavior through the lens of social identity theory [@tajfel1982social; @turner1987rediscovering], we infer that partisan individuals don't merely align with their political in-groups based on shared political preferences. Instead, they project their identity onto these in-groups [@iyengar2012affect; @iyengar2019origins]. This affiliation naturally leads social media users with partisan leanings to prioritize directional goals over accuracy when deciding what news content to share, a behavior in line with the principles of motivated reasoning [@kunda1990case]. Consequently, this bias systematically affects their decisions on what content to share, favoring those aligning with their in-group's views.

Further empirical evidence bolsters our proposition about partisanship being a decisive factor in news sharing behavior on social media, with subsequent implications for the formation of echo chambers. A study examining ideological diversity in shared and consumed news content on Facebook showed that individuals are primarily swayed by their own choices despite a modest decrease in exposure to ideologically diverse content due to Facebook's algorithm [@bakshy2015exposure]. This inclination towards exposure and engagement with politically congruent content emphasizes the user's role in selective sharing. Furthermore, selective sharing has been identified as a catalyst for echo chambers [@bowen2023learning]. Findings suggest that individuals selectively share information even when they are exposed to the same primary information. This shapes the information landscape of their networks, causing belief divergence and polarization. This highlights a strong preference among partisans for sharing like-minded news content.

Selective sharing extends beyond mainstream news, infiltrating both fabricated news or 'fake news' and fact-checking news. Evidence from the 2016 US Presidential elections illustrated this pattern, where it was found that partisan individuals selectively distributed "fake news" that resonated with their political leanings, particularly among conservative voters [@guess2020exposure]. Consequently, unfounded news from questionable sources gained significant traction on social media platforms, amplifying echo chambers and facilitating the spread of misinformation. Other psychological motivators such as cognitive laziness [@pennycook2019lazy], disruptive motivation [@petersen2020need], or partisan directional motivation @taber2006motivated are significant. Yet, it is the partisanship-based motivation that most powerfully influences the sharing behavior of fake news [@osmundsen2021partisan]. Concurrently, empirical support exists for the idea that partisan individuals selectively circulate fact-checking messages that either praise their chosen candidate or denigrate the opposition [@shin2017partisan]. This behavior results in an ideologically biased distribution of fact-checks to their followers. This clear tendency among partisan individuals to selectively share politically compatible content underscores the influence of pre-existing attitudes on information sharing behaviors.



<!-- The question of whether individuals share and are exposed to ideologically diverse information has been a focal point of inquiry among social scientists for some time [@mutz2002cross; @sears1967selective; @iyengar2009red; @stroud2011niche]. The emergence of partisanship as a social identity [@iyengar2012affect; @tajfel1970experiments] and the rapid proliferation of political information sources have further heightened these concerns [@bennett2008new]. It has become increasingly common for people to access news and other political information via social media platforms [@weeks2017incidental; @gil2017effects; @mitchell2016modern]. However, when addressing the question of whether individuals consume and are exposed to a wide array of news and political information through social media, the response is less than encouraging. A body of research utilizing extensive social media usage data suggests that the networks of social media users tend to be ideologically homogeneous [@conover2011political; @barbera2015birds]. The partisan selective sharing of information is seemingly fostering an environment of partisan selective exposure [@bakshy2015exposure; @barbera2015tweeting]. -->

<!-- As the study @bakshy2015exposure shows,  -->

<!-- Assuming that political fact-checking only confirms prior beliefs and is heavily consumed within like-minded groups, we may not expect the real-world implications of numerous experimental findings that demonstrate political corrections work irrespective of the consistency between the content of fact-checking and prior beliefs [@wood2019elusive; @coppock2023conceptual]. Recent studies indeed show that partisans hold starkly different beliefs regarding objective facts, particularly when these facts are tied to their political stance [@jerit2012partisan; @peterson2021partisan]. While some researchers have suggested that such responses are expressive rather than sincere [@bullock2013partisan; @bullock2019partisan], a recent study by @peterson2021partisan detected a significant gap in political misinformation comprehension between partisan groups, even when accuracy motivation was incentivized. Crucially, recent incidents threatening democratic norms---such as allegations of voter fraud and the Capitol attack on January 6th, 2021 [@Cuthbert2022do]---have their roots in uncorrected misperceptions of factual reality. Given this, it is vital not only to understand how political misinformation is disseminated [@vosoughi2018spread; @lazer2018science; @grinberg2019fake], but also to understand the dissemination of political corrective information. However, again, our scholarly comprehension of the latter is significantly limited compared to the former. -->

<!-- Setting aside the last concern about epistemology, which falls outside the scope of empirical examination, we turn to the first concern, commonly known as the 'backfire effect.' This phenomenon was initially identified by @nyhan2010corrections in an experiment where participants were exposed to misinformation regarding Iraq's WMDs and subsequently presented with fact-checking. Intriguingly, the study revealed that individuals with robust prior belief in the misinformation ended up reinforcing their misperceptions following exposure to contradicting fact-checks. This effect has since been extensively scrutinized in a range of subsequent studies, aimed at discerning its existence and prevalence in various scenarios. -->

<!-- Over a decade since the original study by @nyhan2010corrections warning the existence of backfire effect of political correction, the scholarly consensus now points towards the general non-existence of the backfire effect. It is posited that people, *on average*, modify their beliefs about information (or misinformation) to align with fact-checking [@wood2019elusive; @coppock2023conceptual; @swire2017processing].^[The term *on average* implies that the backfire effect can still occur at an individual level, an eventuality that can be gauged using an alternative metric, such as *backfire rate* [see @swirethompson2020searching].] A multitude of experimental studies have underlined that the initial findings by @nyhan2010corrections neither find widespread replication nor broad applicability in diverse contexts [see @wood2019elusive for a thorough review of the evidence]. Crucially, more recent studies affirm the effectiveness of fact-checking in various fields, including politics [@coppock2023conceptual] and health [@carey2022ephemeral], irrespective of respondents' pre-existing attitudes [@coppock2023conceptual] or their perception of source identity [i.e., whether the source is perceived as an in-group or out-group, @chae2023perceiving]. Given that significant amount of studies showed partisans are susceptible to the directional motivation when they are processing politics related (mis)information [@taber2006motivated; @peterson2021partisan], this consistent finding of persuasive effect of correction is optimistic. -->

<!-- To understand this partisan cheerleading pattern we should focus on the recent studies showing the empirical evidence of still remaining partisan motivation related with political fact-checking. One of that, perception of political bias in fact-checking which is originating from the idea of 'hostile media perception'---the belief that fact-checking content is biased against an individual's political in-group---is indeed a well-documented phenomenon, particularly among partisan individuals [@vallone1985a; @chae2023perceiving; @li2022power]. For instance, recent research by @chae2023perceiving showed that even though both Democrats and Republicans update their factual beliefs on political (mis)information to the direction suggested by fact-checking, they still perceive that the fact-checking is politically biased favorably toward out-party, when the verdict of fact-checking is counter-attitudinal or the source of fact-checking is perceived as out-group. The tendency of bias perception on fact-checking was more prominent among Republican; Notably, this bias perception persists even when the source of fact-checking, such as *Reuters*, does not display explicit political affiliations [@budak2016fair]. Conversely, Republicans tend to perceive less bias when they are informed, hypothetically, that the fact-checking was conducted by *Fox News Channel* [@chae2023perceiving]. -->

<!-- This bias perception on fact-checking is complex problem because verifying whether a factual claim is true or not is necessary regardless of it's potential congruency of the verdict toward on side of political ideology considering the scale and the speed of the spread of misinformation and fake news these days [@vosoughi2018spread; @lazer2018science; @grinberg2019fake].  -->



<!-- Encouragingly, despite these perceived biases, both @chae2023perceiving and @li2022power found that such perceptions do not diminish the persuasive effect of fact-checking. In essence, even in the face of perceived political bias, individuals continue to adjust their prior beliefs to align with the direction suggested by the fact-checking article. However, the relevance of this bias perception becomes apparent when considering its impact on real-world sharing behavior surrounding fact-checking [@shin2017partisan]. This suggests that real-world social engagement with fact-checking tends to function more as a form of partisan cheerleading rather than fostering cross-cutting exposure.  -->

# Present Study & Research Questions

Our study delves into the real-world dissemination of political fact-checking via social media by focusing on the operations of *PolitiFact.* Recognized as one of the most active fact-checking bodies in the United States, *PolitiFact* stands out by attributing one of six truthfulness levels‚ÄîPants on Fire, False, Mostly False, Half True, Mostly True, or True‚Äîto every factual claim they evaluate. This practice significantly facilitates our research process. It offers a clear way to pinpoint the political target of the fact-check and to quantify each statement's factual accuracy [as another example see, @mosleh2022measuring]. This enables us to assess the congruency of each fact-checking news article.

We embark on our investigation with three primary research questions in mind: (1) What patterns emerge in the political dimensions of fact-checking, with specific attention to the selection of targets and the congruence of fact-checking results with different political parties (**RQ1**)? (2) how do the ideological leanings of social media users influence the dissemination of political fact-checking news posts, with particular emphasis on any partisan or ideological alignment in the spread of this content (**RQ2**)? and (3) what are the characteristics of social media users who frequently share political fact-checking content, especially in terms of their ideological leanings, and to what extent does their selective sharing behavior correlate with the strength of their political ideologies (**RQ3**)?

<!-- \newpage -->

# Materials and Methods

## Data Collection

For this study, first of all, we amassed all the fact-checking articles written by *PolitiFact*'s fact-checkers ($N = 9,523$) from January 1, 2016, to December 31, 2021. This time period covers significant events such as the 2016 and 2020 presidential elections and the 2018 midterm election. To determine the congruence of each fact-checking article, we manually identified the party affiliation and ideological leanings of each fact-checking target. Specifically, we categorized each unique target into one of four groups: Liberal/Democrat ($n_{\text{target}} = 737; n_{\text{fact-check}} = 2,363$), Conservative/Republican ($n_{\text{target}} = 999; n_{\text{fact-check}} = 3,336$), unknown affiliation ($n_{\text{target}} = 116; n_{\text{fact-check}} = 759$), or non-political ($n_{\text{target}} = 406; n_{\text{fact-check}} = 3,065$).

Fact-checking targets primarily include politicians, politically-related groups or organizations, and social media posts related to political issues. A target was coded as *Liberal/Democrat* or *Conservative/Republican* based on affiliation with the respective political party, official endorsement of the party or its candidates, or a documented donation history favoring that side. Targets that were clearly politically-related but without clear party affiliation based on these criteria were labeled as having an *unknown affiliation.* This category contains a significant proportion of fake news websites, particularly prevalent during the 2020 U.S. Presidential election [e.g., @grinberg2019fake]. Entities not directly involved in the political realm were coded as *non-political*. After this classification, we refined the dataset to include only those fact-checks that targeted specific political figures or groups, thereby excluding fact-checks focused on social media posts or non-political entities. This resulted in a refined set of 6,458 fact-checks pertaining to factual claims made by 1,852 unique targets. 

To determine the congruency of fact-checking with each party or political ideology, we established a coding system. A fact-check was deemed congruent with Liberal/Democratic ideology if the fact-checked claim made by a Liberal/Democratic entity was rated as 'Mostly True' or 'True', or if the claim made by a Conservative/Republican entity was rated as 'Mostly False', 'False', or 'Pants on Fire'. Similarly, we classified a fact-check as congruent with Conservative/Republican ideology if the claim made by a Conservative/Republican entity was rated as 'Mostly True' or 'True', or if the claim made by a Liberal/Democratic entity was rated as 'Mostly False', 'False', or 'Pants on Fire'.

Subsequently, we collected all tweets from the official *PolitiFact* Twitter account (`@PolitiFact`; $N = 35,965$) spanning from January 1, 2016, to December 31, 2021, using the Twitter Academic application programming interface (API). To focus on the fact-checking tweets, we retained only those tweets that relayed fact-checking news articles by *PolitiFact*'s fact-checkers. Each of these tweets contained a shortened URL leading to the original fact-checking news article on the *PolitiFact* website. By restoring these shortened URLs to their original form, we were able to link each fact-checking article's URL with those included in the tweets, culminating in a selection of 8,391 original *PolitiFact* Twitter posts. This process enabled us to create a dataset encompassing each tweet and its corresponding fact-checking article, the fact-checking target, the fact-checking outcome (e.g., Pants on Fire, False, Mostly False, Half True, Mostly True, or True), the fact-check's congruency with each party or political ideology (i.e., congruent with Conservative/Republican or Liberal/Democrat), and the list of users who shared (i.e., retweeted) each fact-checking Twitter post ($N = 185,765$).^[Due to limitations of the Twitter Academic API, we were only able to collect data on a maximum of 100 users who retweeted each Twitter post. However, given that many posts were retweeted by fewer than 100 users, and considering it is unlikely that excluding additional retweeters could introduce systematic bias relevant to our research questions, we proceeded with our analysis using this dataset.]

## Measurement

### Political Ideology of Twitter Users

While the majority of variables that quantify our study's central interests are unambiguous, it's pertinent to detail our methodology for measuring the latent political ideology of Twitter users who shared fact-checking news posts. In short, we employed each user's network structure from our dataset to gauge their latent political ideology, using a method pioneered by @barbera2015birds. This approach hinges on two fundamental assumptions: (1) that social networks demonstrate homophily [@mcpherson2001birds], and (2) that political ideology operates on a unidimensional scale [@rosenthal2017ideology]. This implies that individuals typically form connections with others who share similar political views [@wu2011says; @conover2011political], and these perspectives can be represented on a single left-right axis.

The methodology espoused by @barbera2015birds employs a type of latent space models (or item-response theory models) that considers ideology as a latent variable. This variable can be captured by studying which political elites, $j$, each user, $i$, follows on Twitter. The model interprets the probability of user $i$ following a political account $j$ using the equation:
\begin{equation}
\text{Pr}(y_{ij} = 1 |\alpha_j, \beta_i, \gamma, \theta_i, \varphi_j) = \text{logit}^{-1}\left(\alpha_j + \beta_i - \gamma||\theta_i - \varphi_j||^2\right),
\end{equation}
wherein $\theta_i \in \mathbb{R}$ represents the ideal point of user $i$, $\varphi_j \in \mathbb{R}$ denotes the ideal point of political elite $j$, $\gamma$ serves as a normalizing constant, $\alpha_j$ measures the popularity of political elite $j$, and $\beta_i$ gauges the political interest of user $i$. Our study's quantity of interest in this model is $\theta_i$: the political ideology of Twitter user $i$.

To directly estimate each user's ideal point in our dataset, we would need to collect exhaustive following information for each user ($N = 185,765$). However, the Twitter Academic API's rate limit makes this process inordinately time-consuming. Consequently, we resorted to using the pre-estimated ideal points provided by @barbera2015tweeting. The author had utilized this model to explore the polarization of political communication on Twitter. The ideology scores, $\theta_i$, of each user $i$ were estimated based on the political elites they followed on Twitter. Building upon the work by @barbera2015tweeting, one of the paper's authors periodically updated these estimates with newly sampled Twitter users in 2020 ($N = 64,579,475$). The target users were randomly sampled from those following a minimum of 3 accounts on the elites' account list as of August 2020 [@barbera2015tweeting].^[See [https://github.com/pablobarbera/twitter_ideology/blob/master/2020-update/01-get-twitter-profile-data.R#L26](https://github.com/pablobarbera/twitter_ideology/blob/master/2020-update/01-get-twitter-profile-data.R#L26) for the full list.] For our study, we only retained users from our sample who also appeared within this randomly sampled dataset of Twitter users' ideology scores. This strategy left us with 153,797 users, approximately 83\% of the total retweeters of *PolitiFact* Twitter posts. We concluded that the sampling criteria of @barbera2015tweeting were unlikely to introduce a bias that would significantly affect our measurements or analysis results.

### Partisan Selective Sharing

To ascertain whether a sharing activity constitutes selective sharing, we must first determine: (1) which political side the shared content favors or is congruent with, and (2) the political leaning of the sharer. The first element is clarified through manual coding. We deemed a fact-check congruent with Liberal/Democrats if it either confirmed the veracity of a Liberal/Democrat claim (via 'Mostly True' or 'True' adjudications), or debunked a Conservative/Republican assertion (using 'Mostly False', 'False', or 'Pants on Fire' verdicts). The converse logic was applied to ascertain congruence with Conservative/Republicans. Concerning the second element---determining the political leaning of user $i$---we referred to the ideal scores of political elites (politicians) from the study by @barbera2015tweeting. This study illustrated that the majority of liberal politicians have ideal scores, denoted as $\hat{\varphi}_j$, approximately or less than $-0.5$, while most conservative politicians possess scores around or greater than $0.5$. Leveraging these reference scores, we classified user $i$ as a liberal leaner if their estimated ideology score was equal to or less than $-0.5$ (i.e., $\hat{\theta}_i \leq -0.5$), and a conservative leaner if their estimated ideology score was equal to or greater than $0.5$ (i.e., $\hat{\theta}_i \geq 0.5$). 

With our method to categorize users' ideologies into one of three categories---liberal leaning, conservative leaning, and moderate---we can now discern which sharing behavior constitutes selective sharing for their partisan in-group. We restrict this measure to only liberal-leaning ($n = 46,995$) and conservative-leaning ($n = 4,686$) samples. We can define the selective sharing for an ideologically-leaning user $i$ simply, $S_i = N_{ic}/N_{it}$, where $S_i$ denotes the selective sharing proportion for user $i$. $N_{ic}$ corresponds to the number of shares by user $i$ congruent with their in-party ideology, while $N_{it}$ represents the total number of shares by user $i$. This ratio provides a measure of the proportion of user $i$'s shares that align with their political leaning.


# Results

Addressing **RQ1**, which explores the political dimensions of fact-checking regarding the selection of targets and the congruence of fact-checking results with different political parties, we provide a detailed breakdown of fact-checking verdicts by their party affiliation in Panel A of \autoref{fig:fig1}. A clear observation from this analysis is that more than 60% of fact-checks adjudicate the initial factual claim as false, with a disproportionately high number of these false claims originating from sources associated with the Conservative or Republican Party. This pattern implies that a large portion of fact-checking efforts may clash with the pre-existing attitudes of individuals supporting these politicians or parties. Shifting our attention to Panel B of \autoref{fig:fig1}, we present a temporal analysis of fact-checking that aligns with the ideology or party of the scrutinized political entity, displayed on a monthly basis. Upon analyzing this data, we unearthed two remarkable patterns. Firstly, there is a consistent pattern throughout the entire period under investigation that reveals a larger volume of fact-checks congruent with Liberal/Democrats than with Conservative/Republicans. Secondly, we identified a pattern of increased fact-checking activity leading up to election periods. This surge in activity is particularly amplified for fact-checks that align with the Liberal/Democrats, indicating a potential interaction between the fact-checking industry and the political election cycle.

<!--  This, in turn, could contribute to a heightened perception among Republicans that fact-checking is politically biased [@chae2023perceiving], particularly given the fact that instances where factual claims were found to be false (including mostly false; $n = 3,619$) were approximately twice as frequent as those where claims were judged to be true (including mostly true; $n = 1,697$). -->



\begin{center}
[\autoref{fig:fig1} here]
\end{center}

Delving into **RQ2**, which pertains to the spread of these political fact-checking news posts, we focus on the ideological tendencies of Twitter users who disseminate fact-checking articles. As portrayed in Panel A of \autoref{fig:fig2}, users sharing fact-checking news from *PolitiFact* predominantly lean towards the liberal side of the political spectrum ($N = 153,807$; $M = -0.44$; $SD = 0.94$). Strikingly, when contrasted with the general ideological distribution of Twitter users‚Äîdrawn from a random sample ($N = 64,579,485$; $M = 0.03$; $SD = 0.95$) compiled from continuous data collection by @barbera2015tweeting‚Äîthe users sharing fact-checking news exhibit a strong tilt towards liberal ideologies based on the result of permutation-based mean comparison, $z = 191.71$, $p < .0001$. This skew holds even when we include fact-checking content that concurs with conservative or Republican ideologies. Fundamentally, when it comes to engagement with political fact-checking news, a clear ideological divide exists between liberals and conservatives. While it is known that Twitter users' demographic characteristics have a tendency to skew towards the liberal end of the spectrum [@wojcik2019sizing], this asymmetry in the sharing of political fact-checking remains strongly apparent even when considering this factor.


Panel B of \autoref{fig:fig2} illustrates the distribution of average estimated political ideology among users who shared political fact-checking posts. It separately displays instances of fact-checking congruent with Liberal/Democrat and Conservative/Republican ideologies. The distribution for users sharing Liberal/Democrat congruent fact-checking is found to closely echo the overall distribution of users who disseminated *PolitiFact* fact-checking posts on Twitter. This resemblance indicates that these posts are not commonly shared among users with conservative leanings. In contrast, the distribution relating to Conservative/Republican congruent fact-checking reveals a higher frequency of sharing among users who lean conservatively. Notably, the average ideology scores for sharers of Liberal/Democrat congruent posts ($M = -0.66$, $SD = 0.2$) and Conservative/Republican congruent posts ($M = -0.16$, $SD = 0.65$) exhibit a distinct difference which is confirmed by a permutation-based mean comparison result of $z = 34.962, p< .0001$. The distribution of the latter is both more skewed and sparse.

<!-- Panel B of \autoref{fig:fig2} demonstrates the distribution of the mean estimated political ideology among users who disseminated political fact-checking content, further categorized by the verdicts of the fact-checking articles. Two key trends emerge from the data. Firstly, irrespective of the target or outcome of fact-checking, the bulk of the sharing activity originates from users who identify as politically liberal. Secondly, conservative users primarily share fact-checking content when the results confirm their political viewpoints. This tendency becomes apparent when fact-checking either refutes the assertions made by Democrats or endorses those made by Republicans (when target is Conservative/Republican, $F[5,4716] = 309.6$, $p<.001$; when target is Liberal/Democrat, $F[5,2726] = 86.43$, $p<.001$). These findings are consistent with previous research leveraging data from the 2012 election [@shin2017partisan]. -->

Finally, Panel C of \autoref{fig:fig2} represents temporal fluctuations and partisan disparities in the 'cheerleading' style dissemination of political fact-checking. Due to the considerable discrepancy in the number of fact-checking retweeters between different political ideologies, we normalized the monthly sharing counts by dividing by the highest monthly count within each ideology group separately ($N_{\text{Max: Libs/Dems}} = 131,743; N_{\text{Max: Cons/Reps}} = 14,201$). The data reveals a noteworthy pattern: there are significant surges in partisan-congruent sharing of fact-checking content, especially during election periods. This pattern is more pronounced among Liberal users than their Conservative counterparts. The divergent behavior between these two groups could be partially attributed to the downturn in Conservative/Republican-congruent fact-checking results following the 2016 presidential election, as illustrated in Panel B of \autoref{fig:fig1}. Consequently, Conservative users have fewer fact-checking resources to disseminate with a partisan cheerleading intent, resulting in a decrease in this particular sharing behavior over time.

\begin{center}
[\autoref{fig:fig2} here]
\end{center}

Regarding **RQ3**, Panel A of \autoref{fig:fig3} highlights how the propagation of political fact-checking on Twitter is concentrated among a relatively small subset of users. To facilitate interpretation, we categorized users with $\hat{\theta}_i$ values equal to or exceeding 0.5 as 'Conservative' and those with values equal to or less than $-0.5$ as 'Liberal', as per the empirical distribution of estimated political ideology among U.S. politicians [@barbera2015birds, see Materials \& Methods section]. The Lorenz curve portrayed here elucidates the inequality in sharing behavior; alignment with the diagonal dotted line signifies equal sharing across all users, while closeness to the bottom-right corner points to pronounced inequality. Our exploration reveals that a small percentage of total retweeters dominate the spread of political fact-checking, thereby highlighting a core group of heavy users as influential propagators of this trend. Notably, for Conservative users, we observed a Gini coefficient of 0.6, with a bootstrapped 95% CI $[0.46, 0.73]$. For Liberal users, the Gini coefficient was slightly higher, at 0.66, with a bootstrapped 95% CI $[0.65, 0.67]$. These findings underscore considerable disparities in the sharing of political fact-checking among Twitter users.

Fianlly, Panel B of \autoref{fig:fig3} illustrates the selective sharing behavior of both conservative-leaning and liberal-leaning users. A clear pattern emerges, showing that the strength of a user's political ideology is positively correlated with the extent of their selective sharing behavior, exhibiting a distinct partisan cheerleading-style sharing of political fact-checking on Twitter. For users who lean conservative, we observed a positive correlation between the intensity of political ideology and the proportion of selective sharing. A stronger ideological leaning corresponds to a higher tendency for selective sharing of fact-checking content that aligns with their political ideology. Specifically, the estimated beta coefficient of OLS for conservative users was $b = 0.16$, $SE = 0.006$, $p < .0001$ 95\% CI $[0.15, 0.17]$. This suggests that as conservative-leaning users become more ideologically entrenched, they are significantly more likely to engage in selective sharing behavior. Similarly, liberal-leaning users also exhibited a positive correlation between the intensity of their political ideology and their propensity for selective sharing. However, the strength of this correlation was slightly less pronounced when compared to conservative-leaning users. For liberal users, the estimated beta coefficient was $b = 0.07$, $SE = 0.01$, $p < .0001$, 95\% CI $[0.05, 0.09]$.

\begin{center}
[\autoref{fig:fig3} here]
\end{center}

<!-- \begin{figure}[h] -->
<!-- \centering -->
<!-- \includegraphics[width=.9\textwidth]{../fig/fig2.pdf} -->
<!-- \caption{Distribution of ideology score of Twitter users ($N = 153,807$) who retweeted fact-checking posts compared to overall Twitter users ($N = 64,579,485$).} -->
<!-- \label{fig:fig2} -->
<!-- \end{figure} -->

<!-- \begin{figure}[h] -->
<!-- \centering -->
<!-- \includegraphics[width=1\textwidth]{../fig/fig3.pdf} -->
<!-- \caption{Distribution of mean ideology score of Twitter users who shared the fact-checking post} -->
<!-- \label{fig:fig3} -->
<!-- \end{figure} -->

<!-- \begin{figure}[h] -->
<!-- \centering -->
<!-- \includegraphics[width=1\textwidth]{../fig/merged_fig2.pdf} -->
<!-- \caption{(Panel A) Distribution of ideology score of Twitter users ($N = 153,807$) who retweeted fact-checking posts compared to overall Twitter users ($N = 64,579,485$). (Panel B) Distribution of mean ideology score of Twitter users who shared the fact-checking post} -->
<!-- \label{fig:fig2} -->
<!-- \end{figure} -->

# Discussion


Given the considerable expansion of the fact-checking sector in recent times [@amazeen2020journalistic; @graves2016] and the abundant empirical data supporting the efficacy of fact-checking in influencing perceptions [@walter2020fact; @porter2022political; @porter2022factual; @nyhan2020taking; @chae2023perceiving; @swire2020they; @swire2017processing], there remains a significant void in our comprehension of how fact-checking articles permeate actual social networks. Filling this void is crucial as it may shed light on the conundrum of intensely divergent factual understanding among partisans [@jerit2012partisan; @peterson2021partisan], despite the booming fact-checking industry. In our pursuit to address this, we amassed all political fact-checking articles from *PolitiFact*, connected them to their corresponding Twitter posts, and scrutinized the estimated political ideology of Twitter users who participated in the dissemination of these fact-checking posts. Our empirical analyses led to key insights. (1) A majority of fact-checks adjudicate initial claims as false, heavily originating from Conservative/Republican sources, and fact-checks congruent with Liberal/Democratic views predominate, escalating during election periods. (2) Twitter users who share fact-checking content are primarily liberal, a trend that remains even when conservative-leaning fact-checks are shared. (3) Both liberal and conservative users exhibit partisan cheerleading, selectively sharing fact-checks that align with their respective ideologies. However, conservative users display a stronger tendency towards selective sharing. (4) Sharing of fact-checking content is dominated by a small subset of users, underscoring significant disparities in dissemination behavior on Twitter.

The results from our study furnish substantial insights into the discourse on misinformation and correction and have far-reaching implications. Firstly, the observable abundance of fact-checking verdicts that are unfavorable towards Republicans could potentially clarify why conservatives are more prone to political misperceptions, a phenomenon highlighted by @garrett2021conservatives. In an experimental environment, @garrett2021conservatives discovered that conservatives were less proficient in discerning political misinformation from accurate information relative to their liberal counterparts. This trend remained consistent even when variations in political knowledge or educational levels were accounted for. Our findings suggest that conservatives, on average, may experience an increased cognitive strain in combating directional motivation when processing political misinformation, given that a considerable proportion of fact-checking verdicts might conflict with their perspectives [@chae2023perceiving]. This presents a nuanced problem. While fact-checkers are under no obligation to overly worry about potential political bias or the final balance of verdicts when undertaking fact-checking, provided their conclusions are supported by objective evidence, the perception of political bias in fact-checking among conservatives is an actuality [@chae2023perceiving; @li2022power]. 

At this point, it is essential to clarify a critical point to avoid misinterpretation of our findings. The observation that a significant proportion of *PolitiFact*'s fact-checking aligns more frequently with the Liberal/Democrat viewpoint should not be hastily interpreted as an indication of political bias within the organization. As highlighted by @groeling2013media, determining media bias is not as simple as calculating congruency or favorability towards one political faction; it requires established reference criteria. Though the fact-checking results from *PolitiFact* appear to demonstrate substantial congruency with the Liberal/Democrat perspective, it does not necessarily indicate an ideological bias within the organization. The absence of a reference dataset representing the total population of factual claims to be evaluated hinders such a determination. The observed alignment could theoretically be attributed to various factors, including a higher frequency of untrustworthy information disseminated by Conservative/Republican politicians [e.g., @lasser2022social], possible biases among fact-checkers, or other unconsidered factors, possibly a combination thereof. However, these conjectures extend beyond the boundaries of our study, and our data does not support such discussions. Consequently, readers should be aware that our findings should not be misconstrued as a measure of *PolitiFact*'s potential political bias. Our study emphasizes the distribution and reception of fact-checking information within the public sphere, not the intrinsic ideological alignment of the fact-checking organization.

Secondly, the asymmetry between the two political ideologies in their social interaction (i.e., retweeting) with political fact-checking is indicative of a complex negative outcome resulting from the first observation. @guess2020exposure's revelation, via web-tracking data, that the direct access rate to fact-checking websites among representative samples is incredibly low suggests that incidental exposure to fact-checking posts on social media might account for a significant portion of the real-world consumption of fact-checking content. However, our data shows that conservatives rarely share political fact-checking content, and this behavior only materializes when the verdict aligns with their predispositions. This trend is mirrored among liberal users, which becomes more pronounced during recent electoral periods. If partisans are not exposed to counter-attitudinal fact-checking, then the robust persuasive effect of fact-checking, as substantiated by empirical findings, could be largely theoretical rather than practical. It is pertinent to question the applicability of experimental findings if the scenarios could only counterfactually occur due to enforced exposure to treatment materials during online survey experiments. This invites contemplation on the disparity between controlled experimental settings and the real-world implementation of fact-checking.

Thirdly, the collective findings, which reflect both the low social interaction of conservatives and the atmosphere suggesting that a majority of fact-checks could be unfavorable from a conservative or Republican perspective, could potentially elicit serious concerns, such as attributing a liberal bias to the entire fact-checking industry (at least to *PolitiFact*). Recent studies have experimentally demonstrated that when Republicans are exposed to incongruent fact-checking, they readily perceive political bias from the fact-checking, even when it is delivered by a neutral third-party [@chae2023perceiving; @li2022power]. Drawing a parallel to the politicization of climate change and global warming issues [@mccright2011politicization], where the terms themselves can prime partisan group identity [@guilbeault2018social], it is conceivable that the term 'fact-checking' could already be priming partisan bias among certain groups in the real world. Just as the issue of climate change has become polarized [@druckman2019evidence; @chinn2020politicization], fact-checking may inadvertently be falling into a similar pattern, with some interpreting it through a partisan lens. This carries implications for the perception of the fact-checking industry and the reception of its outputs among different political groups.

## Limitations and Future Study

Our study, while bridging a significant intellectual gap in this field and proposing vital directions for future research, comes with its own set of limitations. Firstly, it is crucial to note that our data depends solely on fact-checking from *PolitiFact*. Multiple other fact-checking organizations operate within the United States, such as *FactCheck.org* and *AP Fact Check*. As such, our sample scope might contain unique features that are specific to *PolitiFact.* However, *PolitiFact* remains one of the most recognized and active fact-checking organizations, providing clear rating-based decisions that greatly facilitate empirical research. Thus, we considered PolitiFact to be a suitable representative for the general trends in fact-checking. Even so, we acknowledge the potential for unique features or bias specific to *PolitiFact*, that may not necessarily be representative of other fact-checking organizations. Therefore, our results should be interpreted as an analysis of the fact-checking landscape as represented by *PolitiFact*, rather than an overarching study of all fact-checking organizations. Future research could benefit from a comparative analysis of multiple fact-checking platforms, examining potential differences and similarities, and investigating whether the observed trends in this study persist across the broader landscape of fact-checking services.


The second limitation we should note is the nature of our dataset. While it provides a wealth of retweet and user information linked to the sharing of fact-checking articles, it is not an absolute measure of exposure to fact-checking content. Our study operates on the assumption that the act of sharing is a proxy for exposure; however, this assumption does not account for those who may read or be exposed to fact-checking content without sharing it. Recent empirical studies have provided substantial evidence of a highly polarized network structure among politically active users on social media platforms like Twitter [@barbera2015tweeting; @barbera2015birds]. This implies that selective sharing patterns could, to some extent, indicate selective exposure, as individuals are more likely to see and engage with content that aligns with their political ideology in these polarized networks. However, this inference does not capture the entirety of the audience for fact-checking content, including silent observers who consume such content without actively sharing it. As such, while our data can provide valuable insights into the dissemination and possible reception of fact-checking articles within certain circles, it may not accurately depict the full spectrum of exposure to this content. Therefore, while our findings offer valuable insights, we caution readers against making definitive assumptions about the broader exposure to fact-checking content based solely on our data regarding sharing behavior. This limitation highlights the need for future research to develop more sophisticated methodologies to accurately measure direct and indirect exposure to fact-checking content across various demographic and ideological groups.


A third limitation may lie in the socio-political demographics of Twitter users. Twitter, as a platform, is not fully representative of the broader population. According to Pew Research Center [@wojcik2019sizing], Twitter users are more likely to be younger, more educated, and more affluent compared to the general public. Additionally, the platform has been observed to have a higher representation of liberal or left-leaning individuals. Consequently, our findings, while applicable to the Twitter sphere, might not be entirely generalizable to other social media platforms or the general public. Our conclusions are therefore specifically focused on the dynamics of fact-checking dissemination within the Twitter ecosystem, and should not be directly extrapolated to other social media platforms or offline interactions without further investigation. Future studies could look at fact-checking dissemination and engagement on different social media platforms, to gain a more comprehensive understanding of these dynamics across different segments of the population.

## Conclusion

In conclusion, our study uncovers important dynamics regarding the dissemination and reception of fact-checking content on Twitter. We reveal an intricate web where political ideology profoundly influences engagement with fact-checking posts, a pattern that might inadvertently narrow the effective reach of these efforts to tackle misinformation. While this paints a complex picture and presents formidable challenges, it also emphasizes the need to refine and reassess our strategies to ensure they remain effective in real-world settings. Indeed, our findings raise critical questions about the actual exposure to and impact of fact-checking efforts in the complex digital landscape. The evident gap between controlled experimental findings, where fact-checking demonstrates robust efficacy, and the reality of selective exposure in highly polarized online social networks, underscores the need to translate experimental results into practical, actionable strategies that work in real-world contexts. 

The challenges outlined in this study provide an opportunity to delve deeper into the nuances of fact-checking efforts, driving future research towards better understanding the intersection of fact-checking, social media dynamics, and political ideologies. In other words, the challenges identified do not denote the failure of fact-checking but highlight areas that need attention for fact-checking to realize its full potential in the age of digital misinformation. The observed patterns and limitations also open avenues for devising strategies that take into account the political ideology of audiences, the perceived political bias in fact-checking, and the actual exposure to fact-checking content on social media platforms. As such, the challenges we uncover can pave the way for a more nuanced, targeted, and hence potentially more effective approach to combating misinformation in the digital age. As we continue to grapple with the global misinformation challenge, it becomes increasingly crucial to understand the real-world dynamics of fact-checking. The future of fact-checking, therefore, lies not just in its capacity to discern truth from falsehood but in its ability to effectively communicate these verdicts across the diverse and complex landscape of online social networks. Ultimately, the success of fact-checking will be measured by its ability to reach and resonate with wide-ranging audiences in our increasingly polarized and digitized era.

\newpage

# References {-}

::: {#refs}
:::

\newpage

\begin{figure}[!ht]
\centering
\includegraphics[width=1\textwidth]{../fig/merged_fig1.pdf}
\caption{Trends in political fact-checking by {\em PolitiFact}. (Panel A) Proportion of fact-checks sorted by the adjudication and party affiliation or political leaning of the target factual claim, conducted by {\em PolitiFact} from January 1, 2016, to December 31, 2021 ($N = 6,458$). (Panel B) Number of fact-checking articles that are congruent with Liberal/Democrats or Conservative/Republicans over time, presented on a monthly basis ($N = 4,598$).}
\label{fig:fig1}
\end{figure}

\newpage

\begin{figure}[!ht]
\centering
    \includegraphics[width=\textwidth]{../fig/merged_fig2.pdf}
    \caption{Distribution of political fact-checking shares via social media. (Panel A) Comparative distribution of ideological scores for Twitter users ($N = 153,807$) who retweeted fact-checking posts and the overall Twitter user base ($N = 64,579,485$). (Panel B) Distribution of mean ideological scores of Twitter users who shared fact-checking posts, subdivided by each adjudication and the party affiliation/political leaning of the fact-check target. (Panel C) Timeline displaying the partisan selective sharing behavior of users from each political ideology (normalized by the maximum month's count in each ideology; $N_{\text{Max: Libs/Dem}} = 131,743; N_{\text{Max: Cons/Rep}} = 14,201$).}
    \label{fig:fig2}
\end{figure}

\newpage

\begin{figure}[!ht]
\centering
    \includegraphics[width=\textwidth]{../fig/merged_fig3.pdf}
    \caption{Analysis of fact-check sharing behavior on Twitter based on user political ideology. (Panel A) A Lorenz curve illustrates the cumulative distribution of political fact-check shares across users, categorized by political ideology. (Panel B) Plot depicts the correlation between political ideology intensity and the proportion of selective fact-check sharing, with separate evaluations for Conservative-leaning and Liberal-leaning users. The fitted lines are estimated using the least squares method, where each unit is weighted according to its log-frequency of sharing.}
    \label{fig:fig3}
\end{figure}



<!-- \newpage -->

<!-- # Appendix -->


